{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e3f6479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Module: null prototype] { default: {}, \u001b[32m\"module.exports\"\u001b[39m: {} }"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import \"dotenv/config\";\n",
    "import { ChatBedrockConverse } from \"@langchain/aws\";\n",
    "\n",
    "const llm = new ChatBedrockConverse({\n",
    "  model: \"us.amazon.nova-micro-v1:0\",\n",
    "  region: Deno.env.BEDROCK_AWS_REGION ?? \"us-east-1\",\n",
    "  credentials: {\n",
    "    secretAccessKey: Deno.env.BEDROCK_AWS_SECRET_ACCESS_KEY ?? \"\",\n",
    "    accessKeyId: Deno.env.BEDROCK_AWS_ACCESS_KEY_ID ?? \"\",\n",
    "  },\n",
    "  temperature: 0,\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba79a705",
   "metadata": {},
   "source": [
    "## Building Blocks: The Augmented LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60e73a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  searchQuery: \"relationship between Calcium CT score and high cholesterol\",\n",
      "  justification: \"To understand how Calcium CT score relates to high cholesterol levels.\"\n",
      "}\n",
      "[\n",
      "  {\n",
      "    id: \"tooluse_Tzh7COltS96f2GENdodXxA\",\n",
      "    name: \"multiply\",\n",
      "    args: { a: 2, b: 3 },\n",
      "    type: \"tool_call\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import { tool } from \"@langchain/core/tools\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "const searchQuerySchema = z.object({\n",
    "  searchQuery: z.string().describe(\"Query that is optimized web search.\"),\n",
    "  justification: z.string(\"Why this query is relevant to the user's request.\"),\n",
    "});\n",
    "\n",
    "// Augment the LLM with schema for structured output\n",
    "const structuredLlm = llm.withStructuredOutput(searchQuerySchema, {\n",
    "  name: \"searchQuery\",\n",
    "});\n",
    "\n",
    "// Invoke the augmented LLM\n",
    "const output = await structuredLlm.invoke(\n",
    "  \"How does Calcium CT score relate to high cholesterol?\"\n",
    ");\n",
    "\n",
    "console.log(output);\n",
    "\n",
    "const multiply = tool(\n",
    "  async ({ a, b }) => {\n",
    "    return a * b;\n",
    "  },\n",
    "  {\n",
    "    name: \"multiply\",\n",
    "    schema: z.object({\n",
    "      a: z.number(\"the first number\"),\n",
    "      b: z.number(\"the second number\"),\n",
    "    }),\n",
    "  }\n",
    ");\n",
    "\n",
    "// Augment the LLM with tools\n",
    "const llmWithTools = llm.bindTools([multiply]);\n",
    "\n",
    "// Invoke the LLM with input that triggers the tool call\n",
    "const message = await llmWithTools.invoke(\"What is 2 times 3?\");\n",
    "\n",
    "console.log(message.tool_calls);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e12d77",
   "metadata": {},
   "source": [
    "## Prompt chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d105a745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial joke:\n",
      "Why did the cat go to the party alone?\n",
      "\n",
      "Because it couldn't meow-tion anyone to come with it!\n",
      "\n",
      "--- --- ---\n",
      "\n",
      "Improved joke:\n",
      "Why did the cat go to the party alone?\n",
      "\n",
      "Because it couldn't meow-tion anyone to come with it—it just couldn't purr-suade anyone to join its feline-tion party!\n",
      "\n",
      "--- --- ---\n",
      "\n",
      "Final joke:\n",
      "Why did the cat go to the party alone?\n",
      "\n",
      "Because it couldn't meow-tion anyone to come with it—it just couldn't purr-suade anyone to join its feline-tion party! But guess what? When the party got really quiet, it found a mouse that was actually a DJ! Turns out, the cat was the only one who could really appreciate the \"silent disco\"!\n"
     ]
    }
   ],
   "source": [
    "import { StateGraph, Annotation } from \"@langchain/langgraph\";\n",
    "\n",
    "// Graph state\n",
    "const StateAnnotation = Annotation.Root({\n",
    "  topic: Annotation<string>,\n",
    "  joke: Annotation<string>,\n",
    "  improvedJoke: Annotation<string>,\n",
    "  finalJoke: Annotation<string>,\n",
    "});\n",
    "\n",
    "// Define node functions\n",
    "\n",
    "// First LLM call to generate initial joke\n",
    "async function generateJoke(state: typeof StateAnnotation.State) {\n",
    "  const msg = await llm.invoke(`Write a short joke about ${state.topic}`);\n",
    "  return { joke: msg.content };\n",
    "}\n",
    "\n",
    "// Gete function to check if the joke has a puchline\n",
    "function checkPunchline(state: typeof StateAnnotation.State) {\n",
    "  // Simple check = does the joke contain \"?\" or \"!\"\n",
    "  if (state.joke?.includes(\"?\") || state.joke?.includes(\"!\")) {\n",
    "    return \"Pass\";\n",
    "  }\n",
    "  return \"Fail\";\n",
    "}\n",
    "\n",
    "// Second LLM call to improve the joke\n",
    "async function improveJoke(state: typeof StateAnnotation.State) {\n",
    "  const msg = await llm.invoke(\n",
    "    `Make this joke funnier by adding wordplay: ${state.joke}`\n",
    "  );\n",
    "  return { improvedJoke: msg.content };\n",
    "}\n",
    "\n",
    "// Third LLM call for final polish\n",
    "async function polishJoke(state: typeof StateAnnotation.State) {\n",
    "  const msg = await llm.invoke(\n",
    "    `Add a surprising twist to this joke: ${state.improvedJoke}`\n",
    "  );\n",
    "  return { finalJoke: msg.content };\n",
    "}\n",
    "\n",
    "// Build wordflow\n",
    "const chain = new StateGraph(StateAnnotation)\n",
    "  .addNode(\"generateJoke\", generateJoke)\n",
    "  .addNode(\"improveJoke\", improveJoke)\n",
    "  .addNode(\"polishJoke\", polishJoke)\n",
    "  .addEdge(\"__start__\", \"generateJoke\")\n",
    "  .addConditionalEdges(\"generateJoke\", checkPunchline, {\n",
    "    Pass: \"improveJoke\",\n",
    "    Fail: \"__end__\",\n",
    "  })\n",
    "  .addEdge(\"improveJoke\", \"polishJoke\")\n",
    "  .addEdge(\"polishJoke\", \"__end__\")\n",
    "  .compile();\n",
    "\n",
    "// Invoke\n",
    "const state = await chain.invoke({ topic: \"cats\" });\n",
    "console.log(\"Initial joke:\");\n",
    "console.log(state.joke);\n",
    "console.log(\"\\n--- --- ---\\n\");\n",
    "if (state.improvedJoke !== undefined) {\n",
    "  console.log(\"Improved joke:\");\n",
    "  console.log(state.improvedJoke);\n",
    "  console.log(\"\\n--- --- ---\\n\");\n",
    "\n",
    "  console.log(\"Final joke:\");\n",
    "  console.log(state.finalJoke);\n",
    "} else {\n",
    "  console.log(\"Joke failed quality gate - no punchline detected!\");\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656b178a",
   "metadata": {},
   "source": [
    "## Parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0768c1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a story, joke, and poem about cats!\n",
      "\n",
      "STORY:\n",
      "Once upon a time in a quaint little village nestled between rolling hills and lush green meadows, there lived a curious and adventurous cat named Whiskers. Whiskers was no ordinary cat; he had a sleek, silver-gray coat that shimmered in the sunlight, and bright green eyes that sparkled with mischief. He belonged to a kind old lady named Mrs. Thompson, who lived in a cozy cottage at the edge of the village.\n",
      "\n",
      "Whiskers was known throughout the village for his daring escapades. While other cats lounged in sunlit spots or chased after butterflies, Whiskers was always on the move, exploring every nook and cranny of the village and beyond.\n",
      "\n",
      "One sunny morning, Whiskers decided it was time for a grand adventure. He slipped out of the cottage, his tail held high with excitement. As he trotted down the cobblestone path, he passed the village bakery, where the sweet aroma of freshly baked bread wafted through the air. Whiskers paused for a moment, his nose twitching at the delightful scent, but he knew his journey was far from over.\n",
      "\n",
      "He ventured past the bustling market square, where villagers haggled over fresh produce and handmade goods. Whiskers paused to observe a group of children playing tag, his eyes narrowing with a playful challenge. He darted across the square, weaving between the legs of the children, who squealed in surprise and laughter.\n",
      "\n",
      "Whiskers' journey took him to the edge of the village, where the forest began. The trees towered like ancient sentinels, their leaves whispering secrets to one another. Whiskers padded softly through the underbrush, his eyes scanning the surroundings for any signs of adventure. He soon stumbled upon a hidden glade, where sunlight filtered through the leaves, creating a magical, dappled light on the forest floor.\n",
      "\n",
      "In the center of the glade stood an old oak tree, its trunk wide and gnarled. Whiskers approached it cautiously, sensing something extraordinary about the tree. As he circled around it, he noticed a small, shimmering object nestled in the roots. Curiosity piqued, he pawed at the object, revealing a tiny, golden key.\n",
      "\n",
      "Whiskers' heart raced with excitement. He had heard tales of a hidden treasure buried deep within the forest, protected by a magical guardian. The key must be the key to unlocking it! But where was the treasure hidden?\n",
      "\n",
      "Determined to find out, Whiskers followed the winding path deeper into the forest, the golden key clutched in his mouth. As he ventured further, he encountered various challenges: a swift-flowing stream, a thicket of brambles, and a steep hill. But Whiskers, with his agility and courage, overcame them all.\n",
      "\n",
      "Finally, after what seemed like hours of exploration, Whiskers arrived at a clearing where an ancient stone pedestal stood, covered in moss and vines. On top of the pedestal was a small, ornate chest, its surface adorned with intricate carvings.\n",
      "\n",
      "With a steady paw, Whiskers placed the golden key into a small slot on the chest. With a soft click, the chest opened, revealing a trove of sparkling jewels, golden coins, and ancient artifacts. But Whiskers knew the true treasure wasn't the riches within; it was the adventure, the discovery, and the spirit of exploration that had brought him there.\n",
      "\n",
      "With the chest now open, Whiskers decided to return home, his heart full of joy and his mind brimming with stories to tell. As he retraced his steps back to the village, he felt a sense of contentment wash over him. Whiskers had found not just a treasure, but a deeper understanding of the world around him and his place within it.\n",
      "\n",
      "When Whiskers finally returned to Mrs. Thompson's cottage, he was greeted with a warm smile and a bowl of his favorite treat—cream. He settled down on his favorite sunlit spot, the golden key safely tucked away, knowing that every adventure awaited just beyond the horizon. And so, Whiskers the adventurous cat continued to explore, his heart forever open to the wonders of the world.\n",
      "\n",
      "JOKE:\n",
      "Sure, here's a light-hearted cat joke for you:\n",
      "\n",
      "Why did the cat sit on the computer?\n",
      "\n",
      "Because it wanted to keep an eye on the mouse!\n",
      "\n",
      "Hope that brought a smile to your face! \n",
      "\n",
      "POEM:\n",
      "In twilight's soft and silent grace,\n",
      "Where shadows weave their mystic lace,\n",
      "There roams a creature, sleek and sly,\n",
      "The feline with the velvet eye.\n",
      "\n",
      "A shadow in the moonlit night,\n",
      "With paws that tread without a sound,\n",
      "They prowl the world in stealthy flight,\n",
      "A sentinel of silent ground.\n",
      "\n",
      "Their whiskers twitch, a silent cue,\n",
      "To senses sharp, to worlds unseen,\n",
      "In every flick, a tale they cue,\n",
      "Of mysteries that lie between.\n",
      "\n",
      "They leap with grace, a silent song,\n",
      "A ballet on the quiet stage,\n",
      "Their eyes, like stars in twilight's throng,\n",
      "Reflect the secrets of the age.\n",
      "\n",
      "In sunlit rooms where warmth does play,\n",
      "They stretch, a lazy, golden ray,\n",
      "A purring symphony that sways,\n",
      "The heart with gentle, soothing rays.\n",
      "\n",
      "They watch, they wait, they know their place,\n",
      "In every corner, every space,\n",
      "A guardian of quiet grace,\n",
      "The cat, in every time and place.\n",
      "\n",
      "Oh, cats, with hearts both wild and free,\n",
      "In every home, you set us free,\n",
      "From worries, from the world's decree,\n",
      "With love that knows no mystery.\n"
     ]
    }
   ],
   "source": [
    "import { StateGraph, Annotation } from \"@langchain/langgraph\";\n",
    "\n",
    "// GraphState\n",
    "const StateAnnotation = Annotation.Root({\n",
    "  topic: Annotation<string>,\n",
    "  joke: Annotation<string>,\n",
    "  story: Annotation<string>,\n",
    "  poem: Annotation<string>,\n",
    "  combinedOutput: Annotation<string>,\n",
    "});\n",
    "\n",
    "// Nodes\n",
    "// First LLM call to generate initial joke\n",
    "async function callLlm1(state: typeof StateAnnotation.State) {\n",
    "  const msg = await llm.invoke(`Write a joke about ${state.topic}`);\n",
    "  return { joke: msg.content };\n",
    "}\n",
    "\n",
    "// Second LLM call to generate story\n",
    "async function callLlm2(state: typeof StateAnnotation.State) {\n",
    "  const msg = await llm.invoke(`Write a story about ${state.topic}`);\n",
    "  return { story: msg.content };\n",
    "}\n",
    "\n",
    "// Third LLM call to generate poem\n",
    "async function callLlm3(state: typeof StateAnnotation.State) {\n",
    "  const msg = await llm.invoke(`Write a poem about ${state.topic}`);\n",
    "  return { poem: msg.content };\n",
    "}\n",
    "\n",
    "// Combine the joke, story and poem into a single output\n",
    "async function aggregator(state: typeof StateAnnotation.State) {\n",
    "  const combined =\n",
    "    `Here's a story, joke, and poem about ${state.topic}!\\n\\n` +\n",
    "    `STORY:\\n${state.story}\\n\\n` +\n",
    "    `JOKE:\\n${state.joke}\\n\\n` +\n",
    "    `POEM:\\n${state.poem}`;\n",
    "  return { combinedOutput: combined };\n",
    "}\n",
    "\n",
    "// Build workflow\n",
    "const parallelWorkflow = new StateGraph(StateAnnotation)\n",
    "  .addNode(\"callLlm1\", callLlm1)\n",
    "  .addNode(\"callLlm2\", callLlm2)\n",
    "  .addNode(\"callLlm3\", callLlm3)\n",
    "  .addNode(\"aggregator\", aggregator)\n",
    "  .addEdge(\"__start__\", \"callLlm1\")\n",
    "  .addEdge(\"__start__\", \"callLlm2\")\n",
    "  .addEdge(\"__start__\", \"callLlm3\")\n",
    "  .addEdge(\"callLlm1\", \"aggregator\")\n",
    "  .addEdge(\"callLlm2\", \"aggregator\")\n",
    "  .addEdge(\"callLlm3\", \"aggregator\")\n",
    "  .addEdge(\"aggregator\", \"__end__\")\n",
    "  .compile();\n",
    "\n",
    "// Invoke\n",
    "const result = await parallelWorkflow.invoke({ topic: \"cats\" });\n",
    "console.log(result.combinedOutput);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16419dae",
   "metadata": {},
   "source": [
    "## Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "052717d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a joke for you:\n",
      "\n",
      "Why did the cat go to the red cross?\n",
      "\n",
      "Because it wanted to be a first-aid kit!\n",
      "\n",
      "Hope that brought a smile to your face!\n"
     ]
    }
   ],
   "source": [
    "import { StateGraph, Annotation } from \"@langchain/langgraph\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "// Schema for structured output to use as routing logic\n",
    "const routeSchema = z.object({\n",
    "  step: z\n",
    "    .enum([\"poem\", \"story\", \"joke\"])\n",
    "    .describe(\"The next step in the routing process\"),\n",
    "});\n",
    "\n",
    "// Augment the LLM with schema for structured output\n",
    "const router = llm.withStructuredOutput(routeSchema);\n",
    "\n",
    "// Graph state\n",
    "const StateAnnotation = Annotation.Root({\n",
    "  input: Annotation<string>,\n",
    "  decision: Annotation<string>,\n",
    "  output: Annotation<string>,\n",
    "});\n",
    "\n",
    "// Nodes\n",
    "// Write a story\n",
    "async function llmCall1(state: typeof StateAnnotation.State) {\n",
    "  const result = await llm.invoke([\n",
    "    { role: \"system\", content: \"You are an expert storyteller.\" },\n",
    "    { role: \"user\", content: state.input },\n",
    "  ]);\n",
    "  return { output: result.content };\n",
    "}\n",
    "\n",
    "// Write a joke\n",
    "async function llmCall2(state: typeof StateAnnotation.State) {\n",
    "  const result = await llm.invoke([\n",
    "    { role: \"system\", content: \"You are an export comedian\" },\n",
    "    { role: \"user\", content: state.input },\n",
    "  ]);\n",
    "  return { output: result.content };\n",
    "}\n",
    "\n",
    "// Write a poem\n",
    "async function llmCall3(state: typeof StateAnnotation.State) {\n",
    "  const result = await llm.invoke([\n",
    "    {\n",
    "      role: \"system\",\n",
    "      content: \"You are an expert poet.\",\n",
    "    },\n",
    "    {\n",
    "      role: \"user\",\n",
    "      content: state.input,\n",
    "    },\n",
    "  ]);\n",
    "  return { output: result.content };\n",
    "}\n",
    "\n",
    "async function llmCallRouter(state: typeof StateAnnotation.State) {\n",
    "  // Route the input to the appropriate node\n",
    "  const decision = await router.invoke([\n",
    "    {\n",
    "      role: \"system\",\n",
    "      content:\n",
    "        \"Route the input to story, joke, or poem based on the user's request.\",\n",
    "    },\n",
    "    { role: \"user\", content: state.input },\n",
    "  ]);\n",
    "  return { decision: decision.step };\n",
    "}\n",
    "\n",
    "// Conditional edge function to route to the appropriate node\n",
    "function routeDecition(state: StateAnnotation.State) {\n",
    "  // Return the node name you want to visit next\n",
    "  if (state.decision === \"story\") {\n",
    "    return \"llmCall1\";\n",
    "  } else if (state.decision === \"joke\") {\n",
    "    return \"llmCall2\";\n",
    "  } else if (state.decision === \"poem\") {\n",
    "    return \"llmCall3\";\n",
    "  }\n",
    "}\n",
    "\n",
    "// Build workflow\n",
    "const routerWorkflow = new StateGraph(StateAnnotation)\n",
    "  .addNode(\"llmCall1\", llmCall1)\n",
    "  .addNode(\"llmCall2\", llmCall2)\n",
    "  .addNode(\"llmCall3\", llmCall3)\n",
    "  .addNode(\"llmCallRouter\", llmCallRouter)\n",
    "  .addEdge(\"__start__\", \"llmCallRouter\")\n",
    "  .addConditionalEdges(\"llmCallRouter\", routeDecition, [\n",
    "    \"llmCall1\",\n",
    "    \"llmCall2\",\n",
    "    \"llmCall3\",\n",
    "  ])\n",
    "  .addEdge(\"llmCall1\", \"__end__\")\n",
    "  .addEdge(\"llmCall2\", \"__end__\")\n",
    "  .addEdge(\"llmCall3\", \"__end__\")\n",
    "  .compile();\n",
    "\n",
    "// Invoke\n",
    "const state = await routerWorkflow.invoke({\n",
    "  input: \"Write me a joke about cats\",\n",
    "});\n",
    "console.log(state.output);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf62ae0",
   "metadata": {},
   "source": [
    "## Orchestrator-Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff1703ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Introduction\n",
      "\n",
      "This report offers a comprehensive overview of the subject matter, highlighting its significance and the rationale behind its study. The structure of the report is meticulously designed to guide the reader through the various facets of the topic in a logical and coherent manner. The sections that follow will delve into the key aspects, providing detailed insights and analysis to ensure a thorough understanding of the subject. The organization is as follows:\n",
      "\n",
      "1. **Background Information**\n",
      "2. **Importance of the Topic**\n",
      "3. **Research Objectives**\n",
      "4. **Methodology**\n",
      "5. **Key Findings**\n",
      "6. **Conclusion and Recommendations**\n",
      "\n",
      "Each section is crafted to build upon the previous one, ensuring a seamless flow of information and a comprehensive exploration of the topic.\n",
      "\n",
      "---\n",
      "\n",
      "### Overview of Scaling Laws\n",
      "\n",
      "Scaling laws in the context of large language models (LLMs) describe the relationship between the model's size, computational resources, and performance. These laws are crucial for understanding how improvements in one area can lead to significant gains in the others. \n",
      "\n",
      "Key points include:\n",
      "\n",
      "- **Model Size**: As the number of parameters increases, the model's capacity to learn and generalize from data typically improves. This is often observed in the form of better performance on benchmark tasks.\n",
      "  \n",
      "- **Computational Resources**: The amount of training data and computational power available directly impacts the model's ability to scale. More resources allow for larger models and more complex training procedures.\n",
      "\n",
      "- **Performance Gains**: The scaling laws suggest that doubling the size of a model often leads to approximately linear improvements in performance, though diminishing returns become noticeable as models grow excessively large.\n",
      "\n",
      "Understanding these scaling laws helps researchers and practitioners design more efficient training strategies and allocate resources effectively to achieve desired model performance.\n",
      "\n",
      "---\n",
      "\n",
      "### Theoretical Foundations\n",
      "\n",
      "The theoretical basis for scaling laws in machine learning models is grounded in several key principles and frameworks. These foundations help explain how model performance improves with increased resources such as data, computational power, and model complexity. \n",
      "\n",
      "#### Universal Approximation Theorem\n",
      "\n",
      "The Universal Approximation Theorem provides a theoretical underpinning for neural networks, stating that a feedforward network with a single hidden layer containing a finite number of neurons can approximate any continuous function to any desired degree of accuracy. This theorem supports the idea that larger neural networks can model more complex functions.\n",
      "\n",
      "#### Capacity and Overfitting\n",
      "\n",
      "The concept of model capacity is crucial in understanding scaling laws. Model capacity refers to the ability of a model to represent complex patterns. As the capacity increases, typically through more parameters, a model can fit the training data more precisely. However, there is a trade-off with overfitting, where the model learns the noise in the training data rather than the underlying distribution. Scaling laws help navigate this trade-off by suggesting that with sufficient data, the benefits of increased capacity outweigh the risks of overfitting.\n",
      "\n",
      "#### Data Efficiency\n",
      "\n",
      "Data efficiency refers to the amount of data required to achieve a certain level of performance. Theoretical foundations suggest that as the size of the model and the amount of data increase, the data efficiency improves. This means that larger models can achieve high performance with relatively smaller datasets, provided the data is sufficiently representative and diverse.\n",
      "\n",
      "#### Computational Scaling\n",
      "\n",
      "Theoretical insights into computational scaling highlight how the performance of machine learning models scales with computational resources. This includes both the time complexity of training algorithms and the efficiency of parallel computing. As computational resources increase, the time required to train larger models can be mitigated, allowing for more complex models to be trained effectively.\n",
      "\n",
      "#### Empirical Validations\n",
      "\n",
      "Empirical validations of scaling laws often involve large-scale experiments where models of varying sizes are trained on datasets of different scales. These experiments provide practical evidence supporting the theoretical foundations, showing that the performance gains predicted by scaling laws are indeed realized in practice.\n",
      "\n",
      "In summary, the theoretical foundations for scaling laws in machine learning are built on principles such as the Universal Approximation Theorem, the balance between model capacity and overfitting, data efficiency, computational scaling, and empirical validations. These foundations help guide the development and optimization of machine learning models.\n",
      "\n",
      "---\n",
      "\n",
      "## Empirical Findings\n",
      "\n",
      "The empirical findings section presents data and research findings that support the scaling laws. The results are derived from a comprehensive analysis of various datasets, including both historical and contemporary data points. \n",
      "\n",
      "### Key Findings\n",
      "\n",
      "- **Data Consistency**: The analysis reveals a strong correlation between the variables under consideration, demonstrating the robustness of the scaling laws across different contexts.\n",
      "  \n",
      "- **Statistical Significance**: The findings show statistically significant results, with p-values below 0.05, indicating that the observed relationships are not due to random chance.\n",
      "\n",
      "- **Trend Projections**: The data supports the predictive power of the scaling laws, allowing for accurate projections of future trends based on current data trends.\n",
      "\n",
      "### Visualization\n",
      "\n",
      "- **Graphs and Charts**: Detailed graphs and charts illustrate the relationships between the variables, highlighting the linear and non-linear trends that align with the scaling laws.\n",
      "\n",
      "- **Heatmaps**: Heatmaps provide a visual representation of the data density and distribution, further validating the scaling laws.\n",
      "\n",
      "### Comparative Analysis\n",
      "\n",
      "- **Benchmarking**: The results are benchmarked against existing models and theories, showing superior accuracy and predictive capability of the scaling laws.\n",
      "\n",
      "- **Case Studies**: Specific case studies demonstrate the practical application and effectiveness of the scaling laws in real-world scenarios.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "The empirical findings strongly support the validity and applicability of the scaling laws, providing a robust foundation for further research and practical implementation.\n",
      "\n",
      "---\n",
      "\n",
      "## Implications\n",
      "\n",
      "The implications of scaling laws on future research and model development are profound and multifaceted. As scaling laws provide a framework for understanding how model performance improves with increased parameters, they offer critical insights for researchers and developers. \n",
      "\n",
      "### Guiding Future Research\n",
      "\n",
      "Scaling laws help prioritize research efforts by identifying the most effective strategies for enhancing model capabilities. By understanding the relationship between model size and performance, researchers can focus on optimizing parameter growth, computational efficiency, and resource allocation. This focus ensures that future advancements are both impactful and sustainable.\n",
      "\n",
      "### Model Development Strategies\n",
      "\n",
      "Scaling laws inform the development of new models by establishing benchmarks for performance improvements. Developers can leverage these laws to predict the outcomes of scaling efforts and to design models that maximize their potential. This predictive capability allows for more strategic planning in model architecture and training methodologies.\n",
      "\n",
      "### Resource Allocation\n",
      "\n",
      "The implications of scaling laws extend to resource management, where they guide the allocation of computational resources. By understanding the diminishing returns on parameter increases, researchers can make informed decisions about where to invest computational power, thereby optimizing the return on investment in model development.\n",
      "\n",
      "### Ethical Considerations\n",
      "\n",
      "Finally, scaling laws also raise important ethical considerations regarding the environmental impact of large-scale model training. As models grow larger, so do their energy consumption and carbon footprint. Future research must consider these implications to develop more environmentally friendly scaling strategies.\n",
      "\n",
      "In summary, scaling laws are not just theoretical constructs; they are practical tools that shape the future trajectory of research and model development, guiding efforts towards more efficient, effective, and sustainable advancements.\n",
      "\n",
      "---\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "This study has highlighted several key findings regarding the impact of renewable energy adoption on urban sustainability. The integration of solar and wind energy has demonstrated significant reductions in greenhouse gas emissions and operational costs. Additionally, the transition to smart grid technologies has enhanced energy efficiency and resilience. \n",
      "\n",
      "Future directions should focus on scaling up renewable energy projects, improving grid infrastructure, and fostering community engagement to ensure sustainable urban development. Continued research into innovative technologies and policy frameworks will be crucial for achieving long-term sustainability goals.\n"
     ]
    }
   ],
   "source": [
    "import { z } from \"zod\";\n",
    "import { Annotation, StateGraph, Send } from \"@langchain/langgraph\";\n",
    "\n",
    "// Schema for structured output to use in planning\n",
    "const sectionSchema = z.object({\n",
    "  name: z.string().describe(\"Name for this section of the report.\"),\n",
    "  description: z\n",
    "    .string()\n",
    "    .describe(\n",
    "      \"Brief overview of the main topics and concepts to be covered in this section\"\n",
    "    ),\n",
    "});\n",
    "\n",
    "const sectionsSchema = z.object({\n",
    "  sections: z.array(sectionSchema).describe(\"Sections of the report.\"),\n",
    "});\n",
    "\n",
    "// Augment the LLM with schema for structured output\n",
    "const planner = llm.withStructuredOutput(sectionsSchema);\n",
    "\n",
    "// Graph state\n",
    "const StateAnnotation = Annotation.Root({\n",
    "  topic: Annotation<string>,\n",
    "  sections: Annotation<Array<z.infer<typeof sectionSchema>>>,\n",
    "  completedSections: Annotation<string[]>({\n",
    "    default: () => [],\n",
    "    reducer: (a, b) => a.concat(b),\n",
    "  }),\n",
    "  finalReport: Annotation<string>,\n",
    "});\n",
    "\n",
    "// Worker state\n",
    "const workerStateAnnotaiton = Annotation.Root({\n",
    "  section: Annotation<z.infer<typeof sectionSchema>>,\n",
    "  completedSections: Annotation<string[]>({\n",
    "    default: () => [],\n",
    "    reducer: (a, b) => a.concat(b),\n",
    "  }),\n",
    "});\n",
    "\n",
    "// Nodes\n",
    "async function orchestrator(state: typeof StateAnnotation.State) {\n",
    "  // Generate queries\n",
    "  const reportSections = await planner.invoke([\n",
    "    { role: \"system\", content: \"Generate a plan for the report.\" },\n",
    "    { role: \"user\", content: `Here is the report topic: ${state.topic}` },\n",
    "  ]);\n",
    "  return { sections: reportSections.sections };\n",
    "}\n",
    "\n",
    "async function llmCall(state: typeof WorkerStateAnnotation.State) {\n",
    "  // Generate section\n",
    "  const section = await llm.invoke([\n",
    "    {\n",
    "      role: \"system\",\n",
    "      content:\n",
    "        \"Write a report seciton following the provided name and description. Include no preamble for each section. Use markdown formatting.\",\n",
    "    },\n",
    "    {\n",
    "      role: \"user\",\n",
    "      content: `Here is the section name: ${state.section.name} and description: ${state.section.description}`,\n",
    "    },\n",
    "  ]);\n",
    "\n",
    "  // Write the updated section to completed sections\n",
    "  return { completedSections: [section.content] };\n",
    "}\n",
    "\n",
    "async function synthesizer(state: typeof StateAnnotation.State) {\n",
    "  // List of completed sections\n",
    "  const completedSections = state.completedSections;\n",
    "\n",
    "  // Format completed section to str to use as context for final sections\n",
    "  const completedReportSections = completedSections.join(\"\\n\\n---\\n\\n\");\n",
    "\n",
    "  return { finalReport: completedReportSections };\n",
    "}\n",
    "\n",
    "// Conditional edge function to create llm_call workers that each write a section of the report\n",
    "function assignWorkers(state: typeof StateAnnotation.State) {\n",
    "  // Kick off section writing in parallel via Send() API\n",
    "  return state.sections.map((section) => new Send(\"llmCall\", { section }));\n",
    "}\n",
    "\n",
    "// Build workflow\n",
    "const orchestratorWorker = new StateGraph(StateAnnotation)\n",
    "  .addNode(\"orchestrator\", orchestrator)\n",
    "  .addNode(\"llmCall\", llmCall)\n",
    "  .addNode(\"synthesizer\", synthesizer)\n",
    "  .addEdge(\"__start__\", \"orchestrator\")\n",
    "  .addConditionalEdges(\"orchestrator\", assignWorkers, [\"llmCall\"])\n",
    "  .addEdge(\"llmCall\", \"synthesizer\")\n",
    "  .addEdge(\"synthesizer\", \"__end__\")\n",
    "  .compile();\n",
    "\n",
    "// Invoke\n",
    "const state = await orchestratorWorker.invoke({\n",
    "  topic: \"Create a report on LLM scaling laws\",\n",
    "});\n",
    "console.log(state.finalReport);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907d93d0",
   "metadata": {},
   "source": [
    "## Evaluator-optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "649c6089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a light-hearted cat joke for you:\n",
      "\n",
      "Why did the cat go to the party alone?\n",
      "\n",
      "Because when it arrived, it realized it was an \"mice\" meeting!\n",
      "\n",
      "Hope that brought a smile to your face!\n"
     ]
    }
   ],
   "source": [
    "import { z } from \"zod\";\n",
    "import { Annotation, StateGraph } from \"@langchain/langgraph\";\n",
    "\n",
    "// Graph state\n",
    "const StateAnnotation = Annotation.Root({\n",
    "  joke: Annotation<string>,\n",
    "  topic: Annotation<string>,\n",
    "  feedback: Annotation<string>,\n",
    "  funnyOrNot: Annotation<string>,\n",
    "});\n",
    "\n",
    "// Schema for structured output to use in evaluation\n",
    "const feedbackSchema = z.object({\n",
    "  grade: z\n",
    "    .enum([\"funny\", \"not funny\"])\n",
    "    .describe(\"Decide if the joke is funny or not.\"),\n",
    "  feedback: z\n",
    "    .string()\n",
    "    .describe(\n",
    "      \"If the joke is not funny, provide feedback on how to improve it.\"\n",
    "    ),\n",
    "});\n",
    "\n",
    "// Augment the LLM with schema for structured output\n",
    "const evaluator = llm.withStructuredOutput(feedbackSchema);\n",
    "\n",
    "// Nodes\n",
    "async function llmCallGenerator(state: typeof StateAnnotation.State) {\n",
    "  // LLM generates a joke\n",
    "  let msg;\n",
    "  if (state.feedback) {\n",
    "    msg = await llm.invoke(\n",
    "      `Write a joke about ${state.topic} but take into account the feedback: ${state.feedback}`\n",
    "    );\n",
    "  } else {\n",
    "    msg = await llm.invoke(`Write a joke about ${state.topic}`);\n",
    "  }\n",
    "  return { joke: msg.content };\n",
    "}\n",
    "\n",
    "async function llmCallEvaluator(state: typeof StateAnnotation.State) {\n",
    "  // LLM evaluates the joke\n",
    "  const grade = await evaluator.invoke(`Grade the joke ${state.joke}`);\n",
    "  return { funnyOrNot: grade.grade, feedback: grade.feedback };\n",
    "}\n",
    "\n",
    "// Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\n",
    "function routeJoke(state: typeof StateAnnotation.State) {\n",
    "  // Route back to joke generator or end based upon feedback from the evaluator\n",
    "  if (state.funnyOrNot === \"funny\") {\n",
    "    return \"Accepted\";\n",
    "  } else if (state.funnyOrNot === \"not funny\") {\n",
    "    return \"Rejected + Feedback\";\n",
    "  }\n",
    "}\n",
    "\n",
    "// Build workflow\n",
    "const optimizerWorkflow = new StateGraph(StateAnnotation)\n",
    "  .addNode(\"llmCallGenerator\", llmCallGenerator)\n",
    "  .addNode(\"llmCallEvaluator\", llmCallEvaluator)\n",
    "  .addEdge(\"__start__\", \"llmCallGenerator\")\n",
    "  .addEdge(\"llmCallGenerator\", \"llmCallEvaluator\")\n",
    "  .addConditionalEdges(\n",
    "    \"llmCallEvaluator\",\n",
    "    routeJoke,\n",
    "    // Name returned by routeJoke : None of next node to visit\n",
    "    { Accepted: \"__end__\", \"Rejected + Feedback\": \"llmCallGenerator\" }\n",
    "  )\n",
    "  .compile();\n",
    "\n",
    "// Invoke\n",
    "const state = await optimizerWorkflow.invoke({ topic: \"Cats\" });\n",
    "console.log(state.joke);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243b3cc1",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb7504df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  HumanMessage {\n",
      "    \"id\": \"dad0b6a0-d147-4932-a865-c3712afe08cf\",\n",
      "    \"content\": \"Add 3 and 4.\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {}\n",
      "  },\n",
      "  AIMessage {\n",
      "    \"id\": \"e6dc28e8-2583-40fc-af6e-2d50944f6e46\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"type\": \"text\",\n",
      "        \"text\": \"<thinking> The user wants to add two numbers, 3 and 4. I will use the 'add' tool to perform this calculation.</thinking>\\n\"\n",
      "      }\n",
      "    ],\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {\n",
      "      \"$metadata\": {\n",
      "        \"httpStatusCode\": 200,\n",
      "        \"requestId\": \"e6dc28e8-2583-40fc-af6e-2d50944f6e46\",\n",
      "        \"attempts\": 1,\n",
      "        \"totalRetryDelay\": 0\n",
      "      },\n",
      "      \"metrics\": {\n",
      "        \"latencyMs\": 874\n",
      "      },\n",
      "      \"stopReason\": \"tool_use\",\n",
      "      \"usage\": {\n",
      "        \"inputTokens\": 621,\n",
      "        \"outputTokens\": 51,\n",
      "        \"totalTokens\": 672\n",
      "      }\n",
      "    },\n",
      "    \"tool_calls\": [\n",
      "      {\n",
      "        \"id\": \"tooluse_6TS9Vf6_Sg-SLlg_DU-4VQ\",\n",
      "        \"name\": \"add\",\n",
      "        \"args\": {\n",
      "          \"a\": 3,\n",
      "          \"b\": 4\n",
      "        },\n",
      "        \"type\": \"tool_call\"\n",
      "      }\n",
      "    ],\n",
      "    \"invalid_tool_calls\": [],\n",
      "    \"usage_metadata\": {\n",
      "      \"input_tokens\": 621,\n",
      "      \"output_tokens\": 51,\n",
      "      \"total_tokens\": 672\n",
      "    }\n",
      "  },\n",
      "  ToolMessage {\n",
      "    \"id\": \"ba273a5f-d49d-45b1-9069-47edd67bcaed\",\n",
      "    \"content\": \"7\",\n",
      "    \"name\": \"add\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {},\n",
      "    \"tool_call_id\": \"tooluse_6TS9Vf6_Sg-SLlg_DU-4VQ\"\n",
      "  },\n",
      "  AIMessage {\n",
      "    \"id\": \"6974c94b-1610-424a-adaa-6b5c5950d813\",\n",
      "    \"content\": \"The sum of 3 and 4 is 7.\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {\n",
      "      \"$metadata\": {\n",
      "        \"httpStatusCode\": 200,\n",
      "        \"requestId\": \"6974c94b-1610-424a-adaa-6b5c5950d813\",\n",
      "        \"attempts\": 1,\n",
      "        \"totalRetryDelay\": 0\n",
      "      },\n",
      "      \"metrics\": {\n",
      "        \"latencyMs\": 697\n",
      "      },\n",
      "      \"stopReason\": \"end_turn\",\n",
      "      \"usage\": {\n",
      "        \"inputTokens\": 698,\n",
      "        \"outputTokens\": 12,\n",
      "        \"totalTokens\": 710\n",
      "      }\n",
      "    },\n",
      "    \"tool_calls\": [],\n",
      "    \"invalid_tool_calls\": [],\n",
      "    \"usage_metadata\": {\n",
      "      \"input_tokens\": 698,\n",
      "      \"output_tokens\": 12,\n",
      "      \"total_tokens\": 710\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import { tool } from \"@langchain/core/tools\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "// Define tools\n",
    "const multiply = tool(\n",
    "  async ({ a, b }: { a: number; b: number }) => {\n",
    "    return a * b;\n",
    "  },\n",
    "  {\n",
    "    name: \"multiply\",\n",
    "    dscription: \"Multiply two numbers together\",\n",
    "    schema: z.object({\n",
    "      a: z.number().describe(\"first number\"),\n",
    "      b: z.number().describe(\"second number\"),\n",
    "    }),\n",
    "  }\n",
    ");\n",
    "\n",
    "const add = tool(\n",
    "  async ({ a, b }: { a: number; b: number }) => {\n",
    "    return a + b;\n",
    "  },\n",
    "  {\n",
    "    name: \"add\",\n",
    "    description: \"Add two numbers together\",\n",
    "    schema: z.object({\n",
    "      a: z.number().describe(\"first number\"),\n",
    "      b: z.number().describe(\"second number\"),\n",
    "    }),\n",
    "  }\n",
    ");\n",
    "\n",
    "const divide = tool(\n",
    "  async ({ a, b }: { a: number; b: number }) => {\n",
    "    return a / b;\n",
    "  },\n",
    "  {\n",
    "    name: \"divide\",\n",
    "    description: \"Divide two numbers\",\n",
    "    schema: z.object({\n",
    "      a: z.number().describe(\"first number\"),\n",
    "      b: z.number().describe(\"second number\"),\n",
    "    }),\n",
    "  }\n",
    ");\n",
    "\n",
    "// Augment the LLM with tools\n",
    "const tools = [add, multiply, divide];\n",
    "const toolsByName = Object.fromEntries(tools.map((tool) => [tool.name, tool]));\n",
    "const agentWithTools = llm.bindTools(tools);\n",
    "\n",
    "import { MessagesAnnotation, StateGraph } from \"@langchain/langgraph\";\n",
    "import { ToolNode } from \"@langchain/langgraph/prebuilt\";\n",
    "import { SystemMessage, ToolMessage } from \"@langchain/core/messages\";\n",
    "\n",
    "// Nodes\n",
    "async function agentCall(state: typeof MessagesAnnotation.State) {\n",
    "  // LLM decides whether to call a tool or not\n",
    "  const result = await agentWithTools.invoke([\n",
    "    {\n",
    "      role: \"system\",\n",
    "      content:\n",
    "        \"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\",\n",
    "    },\n",
    "    ...state.messages,\n",
    "  ]);\n",
    "\n",
    "  return {\n",
    "    messages: [result],\n",
    "  };\n",
    "}\n",
    "\n",
    "const toolNode = new ToolNode(tools);\n",
    "\n",
    "// Conditional edge function to route to the tool node or end\n",
    "function shouldContinue(state: typeof MessagesAnnotation.State) {\n",
    "  const messages = state.messages;\n",
    "  const lastMessage = messages.at(-1);\n",
    "\n",
    "  // If the LLM makes a tool call, then perform an action\n",
    "  if (lastMessage?.tool_calls?.length) {\n",
    "    return \"Action\";\n",
    "  }\n",
    "  // Otherwise, we stop (reply to the user)\n",
    "  return \"__end__\";\n",
    "}\n",
    "\n",
    "// Build workflow\n",
    "const agentBuilder = new StateGraph(MessagesAnnotation)\n",
    "  .addNode(\"llmCall\", agentCall)\n",
    "  .addNode(\"tools\", toolNode)\n",
    "  // Add edges to connect nodes\n",
    "  .addEdge(\"__start__\", \"llmCall\")\n",
    "  .addConditionalEdges(\"llmCall\", shouldContinue, {\n",
    "    // Name returned by shouldContinue : Name of next node to visit\n",
    "    Action: \"tools\",\n",
    "    __end__: \"__end__\",\n",
    "  })\n",
    "  .addEdge(\"tools\", \"llmCall\")\n",
    "  .compile();\n",
    "\n",
    "// Invoke\n",
    "const messages = [\n",
    "  {\n",
    "    role: \"user\",\n",
    "    content: \"Add 3 and 4.\",\n",
    "  },\n",
    "];\n",
    "\n",
    "const result = await agentBuilder.invoke({ messages });\n",
    "console.log(result.messages);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa32195",
   "metadata": {},
   "source": [
    "### Pre-build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "507a6581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  HumanMessage {\n",
      "    \"id\": \"21dde8df-4601-4c48-a060-cefae7fd5dd0\",\n",
      "    \"content\": \"Multiply 3 and 4.\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {}\n",
      "  },\n",
      "  AIMessage {\n",
      "    \"id\": \"1e13ec15-2aaa-4266-babe-cc7d338b425e\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"type\": \"text\",\n",
      "        \"text\": \"<thinking> The user wants to multiply two numbers: 3 and 4. I can use the \\\"multiply\\\" tool to perform this operation. </thinking>\\n\"\n",
      "      }\n",
      "    ],\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {\n",
      "      \"$metadata\": {\n",
      "        \"httpStatusCode\": 200,\n",
      "        \"requestId\": \"1e13ec15-2aaa-4266-babe-cc7d338b425e\",\n",
      "        \"attempts\": 1,\n",
      "        \"totalRetryDelay\": 0\n",
      "      },\n",
      "      \"metrics\": {\n",
      "        \"latencyMs\": 751\n",
      "      },\n",
      "      \"stopReason\": \"tool_use\",\n",
      "      \"usage\": {\n",
      "        \"inputTokens\": 607,\n",
      "        \"outputTokens\": 49,\n",
      "        \"totalTokens\": 656\n",
      "      }\n",
      "    },\n",
      "    \"tool_calls\": [\n",
      "      {\n",
      "        \"id\": \"tooluse_65wj985GSwy4iiBYWGO5JQ\",\n",
      "        \"name\": \"multiply\",\n",
      "        \"args\": {\n",
      "          \"a\": 3,\n",
      "          \"b\": 4\n",
      "        },\n",
      "        \"type\": \"tool_call\"\n",
      "      }\n",
      "    ],\n",
      "    \"invalid_tool_calls\": [],\n",
      "    \"usage_metadata\": {\n",
      "      \"input_tokens\": 607,\n",
      "      \"output_tokens\": 49,\n",
      "      \"total_tokens\": 656\n",
      "    }\n",
      "  },\n",
      "  ToolMessage {\n",
      "    \"id\": \"41f65f8e-9039-4511-957b-1a2870ef5a37\",\n",
      "    \"content\": \"12\",\n",
      "    \"name\": \"multiply\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {},\n",
      "    \"tool_call_id\": \"tooluse_65wj985GSwy4iiBYWGO5JQ\"\n",
      "  },\n",
      "  AIMessage {\n",
      "    \"id\": \"11363cb5-411d-4721-a8dc-26dd523489b4\",\n",
      "    \"content\": \"The result of multiplying 3 and 4 is 12.\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {\n",
      "      \"$metadata\": {\n",
      "        \"httpStatusCode\": 200,\n",
      "        \"requestId\": \"11363cb5-411d-4721-a8dc-26dd523489b4\",\n",
      "        \"attempts\": 1,\n",
      "        \"totalRetryDelay\": 0\n",
      "      },\n",
      "      \"metrics\": {\n",
      "        \"latencyMs\": 550\n",
      "      },\n",
      "      \"stopReason\": \"end_turn\",\n",
      "      \"usage\": {\n",
      "        \"inputTokens\": 686,\n",
      "        \"outputTokens\": 14,\n",
      "        \"totalTokens\": 700\n",
      "      }\n",
      "    },\n",
      "    \"tool_calls\": [],\n",
      "    \"invalid_tool_calls\": [],\n",
      "    \"usage_metadata\": {\n",
      "      \"input_tokens\": 686,\n",
      "      \"output_tokens\": 14,\n",
      "      \"total_tokens\": 700\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n",
    "\n",
    "// Pass in:\n",
    "// (1) an LLM instance\n",
    "// (2) the tools list (which is used to create the tool node)\n",
    "const prebuiltAgent = createReactAgent({\n",
    "  llm: agentWithTools,\n",
    "  tools,\n",
    "});\n",
    "\n",
    "// invoke\n",
    "const result = await prebuiltAgent.invoke({\n",
    "  messages: [\n",
    "    {\n",
    "      role: \"user\",\n",
    "      content: \"Multiply 3 and 4.\",\n",
    "    },\n",
    "  ],\n",
    "});\n",
    "console.log(result.messages);\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
