{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e3f6479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Module: null prototype] { default: {}, \u001b[32m\"module.exports\"\u001b[39m: {} }"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import \"dotenv/config\";\n",
    "import { ChatBedrockConverse } from \"@langchain/aws\";\n",
    "\n",
    "const llm = new ChatBedrockConverse({\n",
    "  model: \"us.amazon.nova-micro-v1:0\",\n",
    "  region: Deno.env.BEDROCK_AWS_REGION ?? \"us-east-1\",\n",
    "  credentials: {\n",
    "    secretAccessKey: Deno.env.BEDROCK_AWS_SECRET_ACCESS_KEY ?? \"\",\n",
    "    accessKeyId: Deno.env.BEDROCK_AWS_ACCESS_KEY_ID ?? \"\",\n",
    "  },\n",
    "  temperature: 0,\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba79a705",
   "metadata": {},
   "source": [
    "## Building Blocks: The Augmented LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60e73a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  searchQuery: \"relationship between Calcium CT score and high cholesterol\",\n",
      "  justification: \"To find information on how Calcium CT score relates to high cholesterol\"\n",
      "}\n",
      "[\n",
      "  {\n",
      "    id: \"tooluse_gMCLvG3MSBGq83_j4Qn-vg\",\n",
      "    name: \"multiply\",\n",
      "    args: { a: 2, b: 3 },\n",
      "    type: \"tool_call\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import { tool } from \"@langchain/core/tools\";\n",
    "import { z } from \"zod\";\n",
    "\n",
    "const searchQuerySchema = z.object({\n",
    "  searchQuery: z.string().describe(\"Query that is optimized web search.\"),\n",
    "  justification: z.string(\"Why this query is relevant to the user's request.\"),\n",
    "});\n",
    "\n",
    "// Augment the LLM with schema for structured output\n",
    "const structuredLlm = llm.withStructuredOutput(searchQuerySchema, {\n",
    "  name: \"searchQuery\",\n",
    "});\n",
    "\n",
    "// Invoke the augmented LLM\n",
    "const output = await structuredLlm.invoke(\n",
    "  \"How does Calcium CT score relate to high cholesterol?\"\n",
    ");\n",
    "\n",
    "console.log(output);\n",
    "\n",
    "const multiply = tool(\n",
    "  async ({ a, b }) => {\n",
    "    return a * b;\n",
    "  },\n",
    "  {\n",
    "    name: \"multiply\",\n",
    "    schema: z.object({\n",
    "      a: z.number(\"the first number\"),\n",
    "      b: z.number(\"the second number\"),\n",
    "    }),\n",
    "  }\n",
    ");\n",
    "\n",
    "// Augment the LLM with tools\n",
    "const llmWithTools = llm.bindTools([multiply]);\n",
    "\n",
    "// Invoke the LLM with input that triggers the tool call\n",
    "const message = await llmWithTools.invoke(\"What is 2 times 3?\");\n",
    "\n",
    "console.log(message.tool_calls);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e12d77",
   "metadata": {},
   "source": [
    "## Prompt chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dd80cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  generateJoke: \"Why did the cat go to the party alone?\\n\" +\n",
      "    \"\\n\" +\n",
      "    \"Because it wanted to be the purr-ty one!\"\n",
      "}\n",
      "{\n",
      "  jokeMaker: \"Why did the cat go to the party alone?\\n\" +\n",
      "    \"\\n\" +\n",
      "    \"Because it wanted to be the purr-ty one!\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import { task, entrypoint } from \"@langchain/langgraph\";\n",
    "\n",
    "// Tasks\n",
    "\n",
    "// First LLM call to generate initial joke\n",
    "const generateJoke = task(\"generateJoke\", async (topic: string) => {\n",
    "  const msg = await llm.invoke(`Write a short joke about ${topic}`);\n",
    "  return msg.content;\n",
    "});\n",
    "\n",
    "// Gate function to check if the joke has a punchline\n",
    "function checkPunchline(joke: string) {\n",
    "  // Simple check - does the joke contain \"?\" or \"!\"\n",
    "  if (joke.includes(\"?\") || joke.includes(\"!\")) {\n",
    "    return \"Pass\";\n",
    "  }\n",
    "  return \"Fail\";\n",
    "}\n",
    "\n",
    "// Second LLM call to improve the joke\n",
    "const improveJoke = task(\"improveJoke\", async (joke: string) => {\n",
    "  const msg = await llm.invoke(\n",
    "    `Make this joke funnier by adding wordplay: ${joke}`\n",
    "  );\n",
    "  return msg.content;\n",
    "});\n",
    "\n",
    "// Third LLM call for final polish\n",
    "const polishJoke = task(\"polishJoke\", async (joke: string) => {\n",
    "  const msg = await llm.invoke(`Add a surprising twist to this joke: ${joke}`);\n",
    "  return msg.content;\n",
    "});\n",
    "\n",
    "const workflow = entrypoint(\"jokeMaker\", async (topic: string) => {\n",
    "  const originalJoke = await generateJoke(topic);\n",
    "  if (checkPunchline(originalJoke) === \"Pass\") {\n",
    "    return originalJoke;\n",
    "  }\n",
    "  const improvedJoke = await improveJoke(originalJoke);\n",
    "  const polishJoke = await polishJoke(improvedJoke);\n",
    "  return polishJoke;\n",
    "});\n",
    "\n",
    "const stream = await workflow.stream(\"cats\", { streamMode: \"updates\" });\n",
    "\n",
    "for await (const step of stream) {\n",
    "  console.log(step);\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
